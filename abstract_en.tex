% !TeX spellcheck = en_GB

\npsection{Abstract}

\textit{Deep learning} (i.e. \textit{machine learning} of \textit{deep artificial neural networks}, mainly in their \textit{strongly-overparameterized regime}) constitutes the capstone of contemporary machine learning from the \textit{accuracy viewpoint}, achieving \textit{state of the art}  results across a wide variety of tasks, and widespread and growing adoption in industry, consumer market, and services. Nonetheless – potentially hindering its application in critical scenarios – the paradigm may suffer from significant weaknesses, among which that of \textit{adversarial (lack of) robustness}: the ability of purposefully-crafted \textit{perturbations} of the inputs to unexpectedly alter the functioning of a \textit{trained} model with respect to the \textit{clean} inputs – and often to the informed expectations of the user – even \textit{catastrophically}.

In the context of \textit{supervised image classification} – on which we will focus in the present work – this may amount to a slight addition of adequately-distributed noise, imperceptible to human sight, to an otherwise \textit{legitimate} and correctly-classified image being able to induce a misclassification with high confidence in a \textit{neural classifier}; or, on the opposite end of the spectrum, an image resembling \textit{white noise} being classified with high confidence as a given class, steerable by the \textit{attacker}.

Given the utmost importance of such vulnerability in the context of \textit{trustworthy artificial intelligence} – to ensure the development of \textit{learning machines} whose output we can trust, and to harden them against tampering and deliberate misuse – the study of these phenomena, with the development of ever new \textit{attacks} and \textit{defences}, has been central to the deep learning research community in the last years. Yet, the field is evolving rapidly and – despite some remarkable results on a \textit{case by case} basis – no universal or definitive solution exists.

In the following work, we propose a novel deep learning architecture and \textit{training and inference methodology}, dubbed \textit{CARSO} (\textit{CounterAdversarial Recall of Synthetic Observations}), devised to defend against \textit{gradient-based} adversarial attacks in the \textit{white box} setting (i.e. with the attacker able to use freely the model, access weights and gradients), as a \textit{pluggable add-on} to an \textit{adversarially-(pre)trained} classifier. Despite requiring additional access to a dataset of \textit{knowingly-unperturbed} images and to an \textit{attack generation mechanism} (a subset of the requirements of adversarial training), the technique is otherwise fully \textit{unsupervised} – allowing it to leverage any large amount of data – whose acquisition process has been deemed trustworthy – with no additional labelling effort.

For the \textit{training phase} – \textit{clean} images, and attacks targeting the pretrained classifier, are gathered. The \textit{internal representation} produced inside the classifier by both sets of images is then used to condition a \textit{conditional variational autoencoder}, learned to have as inputs the actual images producing the representation, and the corresponding \textit{clean} images as outputs (i.e., a copy of the input if unperturbed; that before the application of the adversarial perturbation, otherwise).

This can be considered to be the unsupervised equivalent to the training of a \textit{denoising class-conditional variational autoencoder} for \textit{input purification}.

During inference – the representation produced by a new input in the pretrained classifier is considered. It is subsequently used to condition the repeated \textit{generative sampling} in the decoder, thus obtaining a collection of \textit{candidate denoised images} associated with the same representation (and, by extension, with the original input image). On such collection, the pretrained classifier is finally used \textit{conventionally}, and the resulting \textit{mode class} returned as output.

Loosely inspired by the process of \textit{active memory recollection} during \textit{visual learning} tasks in animals, including primates and men, the technique just described is able to defend against \textit{universal first-order white-box gradient-based} adversarial attacks effectively, and with only slight accuracy loss, in the settings investigated.

With respect to published \textit{iterative adversarial training} and \textit{input purification} approaches (with the same given \textit{type} and \textit{strength} of the attacks used both in training and inference), \textit{CARSO} generally compares slightly favourably as far as \textit{accuracy under attack} is concerned – and competitively nonetheless at worst. Consistently inferior (but still competitive) results are produced only if compared to \textit{officially-reported} accuracies of \textit{AdvGAN}, whose training time – on the other hand – can be optimistically estimated being at least one order of magnitude longer, and also for that reason difficultly reproducible.

A remarkably favourable comparison with published literature – though – is observed when \textit{unforeseen attacks} come into play, i.e. when the attacks used during inference are potentially different (in strength and/or even type) from those seen during training – and whose usual consequence is the compromission of adversarial robustness to a varying extent, not rarely complete (i.e. \textit{close-to-zero} adversarial accuracy). On specific occasions, \textit{CARSO} against unforeseen attacks even shows improvement compared to \textit{adversarial training} towards that precise attack alone.

Finally, the stochastic nature of the \textit{generative sampling}, the non-differentiability of the \textit{mode-selection} operation, and the \textit{competing gradients} arising at the level of the pretrained classifier (used both to produce a representation serving as input to the autoencoder, and as a classifier for the very same autoencoder’s output) concur at making the adversarial attack of the \textit{CARSO} architecture itself a hard, constrained multiobjective optimization problem – effectively shielding \textit{natively} the additional subnetwork it introduces from the same pitfall it addresses in the original classifier.