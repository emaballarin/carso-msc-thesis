% !TeX spellcheck = en_GB

\npsection{The \textit{tools of the trade}}

The aim of the section that follows will be to provide the essential elements of knowledge whose use has been abundant in, and required to better appreciate, the forthcoming -- while also favouring the contextualisation of novel contributions within the broader field of \textit{deep learning}, and that of \textit{adversarial defences} specifically. As such, with no pretence of exhaustiveness, the following pages will focus chiefly on a high-level conceptual overview, along a \textit{drill-down} from the goals of \textit{artificial intelligence} down towards the very specific problems and use-cases considered.


\subsection{ML $\subseteq$ AI}

\textit{Artificial Intelligence} is usually defined as the field that studies tools and methods capable of reproducing higher-level cognitive functions. \textit{I.e.} rational and autonomous reasoning, decision-making and agency, and/or adaptation to complex or previously unseen scenarios. As such, it is an area that lies over a broad variety of disciplines and approaches: from computer science and mathematics, to psychology and philosophy.

The core gnosiological underpinning from which \textit{Machine Learning} and \textit{Deep Learning} move (called \textit{computational cognitivism}) posits that the previously mentioned goals of \textit{Artificial Intelligence} may be reached by reduction to -- though arbitrarily rich and complex -- algorithmic computation: thanks to the tools of mathematical formalisation and statistical-probabilistic reasoning as a way to quantify and operate under uncertainty.

\textit{Machine Learning} can then be defined as the set of rigorous mathematical techniques (spanning \textit{modelling}, \textit{algorithmics}, \textit{statistical/probabilistic learning theory}, optimisation) leading to the development of algorithms (called indeed \textit{learning algorithms}) to extract information from \textit{experience} (provided in the form of \textit{data}) without being explicitly programmed to execute the specific task \textit{learned}\footnote{Such popular definition of \textit{ML} comes from a rephrased quote from \cite{Samuel1959MachineLearning} -- whose author also helped the development of \TeX, the typesetting system which this thesis has been composed with.}.

In a more measurable fashion\footnote{Such definition is attributed to Tom Mitchell.}:

\begin{displayquote}
    \textit{A computer program is said to learn from experience E \wrt to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.}
\end{displayquote}

Specifically, in order to have a \textit{program} show such kind of behaviour at a given task, it is necessary for the \textit{learning algorithm} itself to learn from data a \textit{mathematical model} of the essential phenomena involved in the fulfilment of the task -- and whose use allows for its (eventually approximate) further execution, even on new, unseen input data. This clarification allows -- in theory -- to decouple the machine learning \textit{model} from the \textit{learning} (and potentially \textit{inference}) \textit{algorithm} used to determine its determining parameters.

\subsubsection{Minimal systematics}
At this point, one can preliminarily classify the (extremely varied, and often not clear-cut) landscape of machine learning tasks (and related algorithms) according to the amount of \textit{supervision} required by the learning, or \wrt to the probability distribution the \textit{model} is tasked to learn.

The former discrimination allows us to define:
\begin{itemize}
    \item \textit{Supervised} learning tasks, requiring an input/output mapping to be learned from a \textit{training} dataset of knowingly-correct pairs of the same kind (\textit{e.g.} image classification);
    \item \textit{Unsupervised} learning, where data are provided as input and properties or transformations of them are required to be learned without further exemplification of the output (\textit{e.g.} dimensionality reduction);
    \item \textit{Reinforcement learning}, involving the determination of the optimal actions (among many) to be taken according to the state in which the \textit{agent} and the \textit{environment} are -- by utilising only a \textit{reward function}, and eventually under the further constraint of partial state observability and outcome nondeterminism.
\end{itemize}

Whereas, according to the latter description, we can have:
\begin{itemize}
    \item \textit{Generative} learning, dedicated to the modelling of the full data-generating distribution, \textit{i.e.} $p(x)$ in the case of inputs only, or the joint $p(x,y)$ in the case of input/output pairs -- or some property or transformation of them;
    \item \textit{Discriminative} learning, dealing with the less informative conditional model of $p(y|x)$ -- or some statistic of it -- in the case of input/output pairs.
\end{itemize}

\subsection{Deep Learning}

Given the above, one can simply define \textit{deep learning} as a subset of \textit{machine learning} -- whose models are \textit{deep artificial neural networks} (whose precise nature will be right introduced).

\subsubsection{From artificial neurons...}

The essential building block of an \textit{artificial neural network} -- being it \textit{shallow} or \textit{deep} -- is the \textit{artificial neuron}. Though not a proper model of its biological counterpart -- unless at a very high, conceptual level -- its structure was loosely inspired\footnote{See \cite{McCullochPitts1990ALC} and \cite{Rosenblatt1958ThePA}.} by the dendritic and axonal connectivity of neurons in a biological brain, abstracting away the differentiations that may occur.

From a mathematical modelling viewpoint, an \textit{artificial neuron} is just an affine vector-to-scalar transformation followed by a (usually, except in the trivial case) nonlinear function, called \textit{activation}.

\textit{i.e.}

$$y = \mathcal{N}_1(\vec{x}) = \mathcal{A}(b + \vec{w} \cdot \vec{x})$$

with, in the most general setting $y \in \mathbb{R}$ (the output), $\vec{x} \in \mathbb{R}^n$, $n \in \mathbb{N}_{\setminus0}$ (the vector input, also viewable as an ordered collection of scalar inputs, \textit{e.g.} the outputs of other \textit{neurons}). In such case, the activation $\mathcal{A}: \mathbb{R} \rightarrow \mathbb{R}$ and the model \textit{parameters} $b \in \mathbb{R}$ (\textit{bias}) and $\vec{w} \in \mathbb{R}^n$ (weights) -- to be learned, eventually -- fully define the model. In the usual scenario, the choice of $\mathcal{A}$ is not to be learned, but still its fixed functional form may depend on additional learnable parameters.

\subsubsection{...to deep artificial neural networks}

If we consider, at this point, a group of $N_{\ell}$ ordered (and distinct) neurons $\mathcal{N}_i$ and provide them the same vector $\vec{x}$ as input, we obtain as output $y_i = \mathcal{N}_i(\vec{x}) = \mathcal{A}_i(b_i + \vec{w}_i \cdot \vec{x})$ for $i \in \{1, \dots, N_{\ell}\}$, which we can rearrange in a vector $\vec{y}$. The transformation mapping $\vec{x}$ into $\vec{y}$ can be directly modelled by means of matrix-vector multiplication, thus defining a new mathematical device, called \textit{linear (neural network) layer}:

$$\vec{y} = L(\vec{x}) = \mathcal{A}(\vec{b} + \mat{W}\vec{x})$$

with $\vec{y}, \vec{b} \in \mathbb{R}^m$, $\vec{x} \in \mathbb{R}^n$, $m,n \in \mathbb{N}_{\setminus0}$ and $\mat{W}$ an adequately-defined $m \times n$ matrix. In this case, $\mathcal{A}: \mathbb{R}^m \rightarrow \mathbb{R}^m$ is usually (but not always\footnote{See \cite{Xiao2020Enhancing} as a refreshing example of such kind.}!) just an elementwise application of the same scalar function.

By considering again a group of $N$ ordered (and distinct) \textit{layers} -- the $j^{\text{th}}$ of which taking as input the output of the $(j-1)^{\text{th}}$ -- we can finally define an \textit{N-layers deep fully-connected feedforward\footnote{\label{graphrepr}Given the absence of \textit{loops} in the (oriented) graph-based representation of the network. There, pre-activations are represented at the \textit{neuron} level one scalar per node; outgoing edges imply the application of the \textit{activation function} and multiplication with the scalar weight, whereas incoming edges, summation. Biases are encoded by the weight of additional edges with constant unit output. The set of all neurons with edges incoming into another (excluding biases) are called \textit{receptive field} of the latter.} artificial neural network} $\mathcal{N}_N$ as:

$$\mathcal{N}_N(\vec{x}) = L_N(L_{N-1}(\dots (L_2(L_1(\vec{x}))))) \text{ .}$$

The output (or \textit{post-activation}) of the $j^{\text{th}}$ layer (\textit{i.e.} $\vec{r_j} = L_j(\vec{r_{j-1}})$) -- or, according to a different convention, its \textit{pre-activation} (\textit{i.e.} $\vec{b_j} + \mat{W_j}\vec{r_{j-1}}$) -- is given the name of \textit{$j^{\text{th}}$-layer representation}. The same naming convention can be scaled down at the neuron level, or up to an entire network (by considering the ordered representations of all composing layers/neurons).

Additionally, all the weights and biases of an artificial neural network (considered as part of an ordered collection) are called -- as in the general statistical, \textit{ML}, or modelling setting -- \textit{parameters} of the model; on the other hand, all the additional characterising elements described by a quantitative choice (or even non-quantitative, \textit{lato sensu}) among many options -- not meant to be learned -- are called \textit{hyperparameters} (among which, \textit{e.g.}, the number of layers -- also called \textit{depth} of the network -- or the output size of each \textit{layer}).

\subsubsection{A whole \textit{bestiary} of networks and \textit{universal approximation}}

At this point, one may ask whether \textit{fully-connected feedforward artificial neural networks (FCNs)} are an adequate model to approximate the transformation of the inputs required by the task to be learned -- or if other kinds of similar models are available (and more appropriate).

As far as the latter question is concerned -- even if not strictly required for the prosecution of the present work -- many variations have been proposed since the first \textit{neural models}. Some of these act at the level of \textit{single neurons} -- \textit{e.g.} by changing the way incoming inputs in the graph-representation (see: {\ref{graphrepr}}) of the network are handled, as in \textit{spiking neural networks}; others change the layer-wide behaviour -- \textit{e.g.} by structuring neuron connectivity and enforcing weight-sharing, as in \textit{convolutional neural networks}. Others more change the connectivity of the network in ways that go beyond single layers (as in \textit{recurrent} or \textit{graph} \textit{neural networks}, where -- among other differences -- \textit{e.g.} \textit{feedbacks} are possible). Some of these cases -- and definitely others -- also require (or actually propose, as the only modification) a different \textit{learning algorithm}\footnote{The main \textit{learning algorithm} for deep neural models will be discussed in the following section.} to be employed \wrt already established choices.

Surely -- and beyond the simple variation of hyperparameters -- ever new artificial neural models are routinely developed within deep learning research and practice, by variously composing already established ones (\textit{i.e.} considering the outputs, or even more generally the representations or the parameters, of a network as the input of another), and by likewise adopting different training or inference strategies\footnote{Of this latter kind is main element of novelty of this thesis.}.

The adequacy of deep learning models to approximate a given map linking inputs and outputs of interest -- and specifically whether such approximation can be learned (and how!) from examples -- is a vast subject with rarely clear-cut answers, constantly evolving and attracting renovated research interest.

The main results so far -- known as \textit{universal approximation theorems} -- establish for a given \textit{artificial neural architecture}, seen as a family of algorithmically-generated functions parametrised by its weights and biases, their density within a function space of interest. Originally mostly focused on \textit{feedforward} architectures of fixed depth at the increase of width, spaces of continuous functions between Euclidean spaces, and the notion of \textit{density} induced by uniform convergence within compact sets -- over the years many extensions to such theorems have been proposed and proven (most notably in the case of fixed width and increasing depth, and for a variety of commonly used neural architectures such as \textit{convolutional neural networks} or those with a wide class of activation functions or subject to specific constraints).

In any case -- though the striking expressive power of deep artificial neural networks is undoubtedly confirmed by experimental results -- such theorems are almost always \textit{existence} ones, without providing a direct way of determining the (hyper)parameters actually approximating a given function within stated tolerance. Within the frame just described, the practical determination of such latter (hyper)parameters has largely been an empirical science -- relying upon clever modelling choices (\textit{e.g.} the choice of \textit{inductive biases} -- \textit{i.e.} the properties of specific architectures \wrt the input/output mapping produced), the development of ever more effective learning algorithms, and always experimental evaluation.

It will go beyond the scope of this thesis to discuss in further \textit{depth} or \textit{width} the current state of research in the field. Some cornerstone results are contained in \cite{HornikEtAl1989Approx}, \cite{Hornik1991Approx}, \cite{LinEtAl2018Approx}, \cite{Xu2018ApproxGNN}, and \cite{KidgerLyons2020Approx}.


\subsubsection{\textit{Actually learning} in Deep Learning}

We introduced \textit{deep learning} as a subset of \textit{machine learning} -- however, we have so far outlined just the \textit{modelling} part of it, and briefly mentioned some formal guarantees whose transfer into practice is hardly possible. How can we \textit{automatically} learn the the parameters of a deep learning model capable of approximately performing a given task (or at least give us the reasonable expectation it could) -- only from inputs (\textit{e.g.} in the case of unsupervised learning) or input/output pairs (\textit{e.g.} in the case of supervised learning)?

The answer is, in principle, exactly equivalent to that typical of \textit{traditional} numerical function approximation, or statistical model fitting.

Given a deep artificial neural network $\netw{N}$, we first define an adequate \textit{loss function} -- that should encode the degree of success with which the model is capable of solving the chosen task, and whose value generally depends on inputs and parameters (usually, but not exclusively, through $\netw{N}$) and, in the case of supervised learning, on the outputs. Then, we minimise the \textit{loss}, optimising \wrt the parameters, while evaluating it on the given \textit{training data}.

More precisely, calling $\vec{\theta}$ the collection of weights and biases for the entire model $\netw{N}$, and $\mathcal{L}_{\netw{N}}$ the chosen loss function, we seek the optimal parameters

$$\vec{\theta^{\star}} \coloneqq \argmin_{\vec{\theta}} \mathcal{L}_{\netw{N}}(\vec{x}, \vec{y} | \vec{\theta}) \fullstop$$

Such simple formulation, however, hides one of the most relevant differences between deep learning and more traditional \textit{(approximate) model fitting} approaches: the parameter space can be extremely high-dimensional\footnote{\textit{E.g.}, among the largest \textit{deep neural models} to date, Google's \texttt{GLaM} -- see \cite{DuEtAl2022GLaM} -- boasts $>1$ trillion learnable parameters!}, and the \textit{loss landscape} -- \textit{i.e.} $\mathcal{L}_{\netw{N}}(\vec{\theta})$ -- highly nonconvex, \textit{rugged}, with abundant of local minima and/or saddle points\footnote{For an impactful visualisation, see \cite{LiEtAl2018VisualizingLL}.}. This rules out any direct global optimisation approach, for problems beyond \textit{toy examples}.

Though not necessarily the \textit{only} choice -- the almost-totality of learning algorithms for deep neural models is based upon \textit{gradient descent iterations}, \textit{i.e.} the approximation $\vec{\theta^{\star}} \approx \vec{\theta_{\hat{i}}}$ for a sufficiently large $\hat{i}$, and the following iteration step:

\begin{equation} \label{eq:1}
    \vec{\theta_{i+1}} \leftarrow  \vec{\theta_{i}} - \lambda\vec{g_i}
\end{equation}

with $\lambda \in \mathbb{R}_+$ (\textit{learning rate}) and $\vec{g_i} \coloneqq \left.\frac{\ddiff{\mathcal{L}_{\netw{N}}(\vec{\theta})}}{\ddiff{\vec{\theta}}} \right|_{\vec{\theta_{i}}}$ (\textit{local gradient}).

The exact dependence of $\vec{g_i}$ from $\vec{x}$ directly relates to the specific choice of an \textit{aggregation scheme} of the elementwise $\vec{\theta_{i}}$-gradients computed for each datapoint available.

Let us suppose the usual \textit{supervised learning} setting, with $\{(\vec{x_i}, \vec{y_i})\}_{i = \{1,2,\dots,N\}}$ as the \textit{training dataset}. $\vec{g_i}$ iterations can then be defined as\footnote{An equivalent formulation is also possible -- with summation replaced by averaging of the gradients -- implying a rescaling of the learning rate.}

$$\vec{g_i} \coloneqq \sum_{k \in \mathcal{B}_j \subseteq \{1,2,\dots,N\}}{\left.\frac{\ddiff{\mathcal{L}_{\netw{N}}(\vec{x_k}, \vec{y_k}|\vec{\theta})}}{\ddiff{\vec{\theta}}} \right|_{\vec{\theta_{i}}}}$$

with simultaneous updates of $i$ and $j$ (\textit{i.e.} for two consecutive iterations, summation is performed on different $\mathcal{B}_j$s), and $\bigcup_j\mathcal{B}_j$ a partition of the (generally shuffled) set of indexes $\{1,2,\dots,N\}$.

The various $\{(\vec{x_i}, \vec{y_i})\}_{i \in \mathcal{B}_j}$s are called \textit{(mini)batches} of the dataset, and their size (fixed, except eventually for last one) $B$ (\textit{batch size}) is such that $\#\text{batches} = \lceil \frac{N}{B}\rceil$ -- whereas the corresponding $\vec{g_i}$s are called \textit{noisy local gradients} in case $B \neq N$ (as they are, indeed, a \textit{noisy estimate} of the actual \textit{local gradient}).

The choice of $B$ and $\lambda$ (which are additional \textit{hyperparameters}, of the learning algorithm this time) can influence the convergence of the iterations. While it is true that \textit{noisy gradient} iterations approximate those of true gradients as $i$ grows, the choice of a larger $B$ (up to the limit $B=N$, called \textit{(full) batch gradient descent}) reduces the variance of the estimate at the cost of increased susceptibility to convergence toward local minima. On the other hand, a decrease in $B$ (down to $B=1$, \textit{stochastic gradient descent}\footnote{Note, however, that such name is commonly used in practice to describe the whole family of these optimisation methods, regardless of the choice of $B$.}) favours convergence toward the global minimum at the price of increased variance and number of iterations required to reach it. The number of times the training set (in the form of batches, eventually) is entirely used during training is called \textit{number of epochs}. Ultimately, the choice of \textit{batch size} in modern day boils down to the compromise\footnote{See, \textit{e.g.} \cite{MastersLuschi2018RevisitingSB} for an unusual take on the subject,  \href{https://twitter.com/ylecun/status/989610208497360896?lang=en}{endorsed} by deep learning pioneer and expert Yann LeCun.} between \textit{regularisation} of the optimisation problem and (reduction in) the number of iterations potentially required for proper convergence -- provided in any case sufficient memory to store the whole minibatch\footnote{While it is always possible to resort to \textit{gradient accumulation} to counter memory starvation (see \href{https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903/20}{this forum comment by one of the PyTorch developers} as an explanatory example), a memory-fittable alternative has to be preferred, due to the non-summability of \textit{batch normalisation} statistics. The analysis of such regularisation technique will be discussed right next.}.

Finally, it must be stressed that \textit{optimisation} plays a crucial role in the \textit{learning algorithm} of a deep neural architecture: for that reason, many variations of the prototypical iteration described in (\ref{eq:1}) have been proposed. Chiefly -- they augment such iteration with additional \textit{iteration variables} (and subsequent additive terms to the gradient) in order to better exploit \textit{greater-than-first} order information about the \textit{loss landscape} and thus provide faster and/or more accurate convergence to the \textit{true} global minimum. A thorough description of such proposals is again out of the scope of this work: however, the most common device used in this context must be mentioned. It is the case of \textit{exponentially-weighted averages} of the gradient. As an example, the nowadays ubiquitous \textit{stochastic gradient descent with momentum} variation proposes a simultaneous iteration of the type:

$$\vec{m_{i+1}} \leftarrow  \vec{\theta_{i+1}} - \vec{\theta_{i}}$$
$$\vec{\theta_{i+1}} \leftarrow  \vec{\theta_{i}} - \lambda\vec{g_i} + \beta\vec{m_i}$$

with $\beta \in (0,1]$. Such addition -- mimicking the \textit{momentum}, indeed, of a body moving subject to a potential $\mathcal{L}_{\mathcal{N}}$ -- \textit{e.g.} both provides noise attenuation for the gradient estimates and ameliorates the convergence towards global minimum in the case of highly unbalanced (in absolute value) gradient components across dimensions. Additionally, \texttt{Adam}\footnote{See \cite{KingmaBa2015Adam}. Also, \texttt{Adam} is the basis for the main optimiser used in the learning algorithm proposed by this work -- \texttt{RAdam} (see \cite{LiuEtAl2020OnTheVariance}). The improvement upon \texttt{Adam} consists in an accurate analytical \textit{de-biasing} of the \textit{adaptive learning rate} variance, especially in the first few iterations of the algorithm, similarly to the previously-known \textit{warmup} heuristic.} -- one more among the most relevant and used variations -- further regularises learning via modulation of the \textit{effective learning rate} in accordance to better estimates of local curvature.

\subsubsection{\textit{Overparametrisation} and regularisation strategies}

As we have anticipated earlier, the number of parameters of a deep neural model can be even extremely large -- and usually purposefully so: excessive parameter parsimony may in fact artificially cap model expressiveness beyond the (usually unknown beforehand) requirements of the task to be learned, and render optimisation by gradient descent harder\footnote{Such latter statement has been heuristically explained by Terrence Sejnowski as follows: the probability that no direction of the \textit{local gradient} points towards a pre-set point -- in this case, the global optimum -- decreases as the dimension of the parameters space increases, even for a randomly-picked \textit{local gradient} vector.}.

Still, the deliberate modelling choice of employing a number of parameters (much) larger than reasonably estimable -- so-called \textit{overparametrisation} -- does come at the cost of increased risk of overfitting\footnote{The loss-minimisation-driven adaptation of the model to the training dataset at the point of losing generalisation ability -- \textit{i.e.} the inability to learn the abstract task beyond the specific exemplifying data.} and unmanageable complexity. To counteract such downsides -- as in \textit{traditional} statistical practice -- \textit{regularisation strategies} have been developed, some of them specific to deep learning, which will be discussed next.

\paragraphnl{Weight decay}
First of all, one could apply -- to the optimisation problem of finding $\vec{\theta^{\star}}$ -- the same \textit{penalisation} regularisation techniques typical of \textit{traditional} statistics; in particular, $L_2$-penalised fitting (\textit{à la} ridge regression). Such approach is called -- within the deep learning community -- \textit{weight decay}\footnote{See \cite{KroghPalmer1991WeightDecay}.}; it is indeed possible, for iterations of the type described in (\ref{eq:1}) to express the $\mathcal{L}_{\text{ridge}} \leftarrow \mathcal{L} + \frac{\gamma}{2}\normof{\vec{\theta}}_2$ regularisation as a modified iteration, \textit{i.e.}

$$\vec{\theta_{i+1}} \leftarrow  \vec{\theta_{i}} - \lambda\vec{g_i} - \lambda\gamma\vec{\theta_i}\fullstop$$

The two routes, however, become different in the cases -- outlined above -- where exponentially-weighted averaging, too, comes into play at the optimisation step by means of a more sophisticated scheme. This previously unnoticed detail has since then produced, once again, different variations of previously known optimisers\footnote{See, \textit{e.g.}, the most relevant optimiser of this kind, \texttt{AdamW} -- described in \cite{LoshchilovHutter2018AdamW}.} -- with an empirical general preference for proper \textit{weight decay}, but no clear-cut conclusion (and a relatively modest effect, compared with other \textit{tricks}).

\paragraphnl{\textit{Dropout}}
Another\footnote{See \cite{SirvastavaEtAl2014Dropout}.} -- \textit{deep-learning-specific} -- technique directly attempts model fitting with a stochastically-selected subset of parameters: for each batch of training data, some randomly-sampled neuron representations (in a given pre-set proportion, layerwise) are forced to zero, resulting in the corresponding weights and biases to be unmodified by the learning iteration. At \textit{inference time}, no constraint is imposed, but each \textit{weight} is further weighted by the corresponding complementary probability of \textit{training time} zeroing.

Such technique -- and its more advanced variations\footnote{See \textit{e.g.} \cite{BolukiEtAl2020BernoulliDropout}.} -- have demonstrated remarkable success in improving generalisation, by taking into account in a less intertwined fashion the various different \textit{pathways} activated through the model by a given input example -- thus increasing the overall parameter efficiency of the network. This comes at the cost of a generally less \textit{sparse} and more \textit{distributed} representation -- which may not always be an intended goal of the training.

\paragraphnl{\textit{Batch Normalisation}}
Though not properly considered a \textit{regularisation technique}, \textit{batch normalisation} has a twofold effect: speeding up and improving converge to the \textit{true} global minimum for practically any optimiser, and reducing the \textit{noisiness} of batched input data -- by normalising each datapoint coordinate within its respective batch, and further scaling and shifting it according to further learnable parameters.

Initially considered to due its efficacy to a reduction in so-called \textit{internal covariate shift}\footnote{See the original paper, \textit{i.e.} \cite{IoffeSzegedy2015BatchNorm}.}, it has been later established\footnote{\cite{SanturkarEtAl2018BatchNorm}.} that its main contribution is a net smoothing effect on the loss landscape.

\paragraphnl{Learning rate scheduling}
Finally, in the same family of \textit{improper regularisation techniques} as \textit{batch normalisation}, there is \textit{learning rate scheduling}: the adaptation of \textit{epoch-} (or, more generally \textit{batch-}) \textit{specific} learning rate to some pre-set or adaptive schedule. Such technique, originally developed for \textit{non-learning-rate-adaptive} optimisers, further speeds up convergence -- while simultaneously avoiding that excessively large steps during the initial iterations of the algorithm steer convergence away from the global minimum, and during the later stages prevent convergence to narrower, but optimal, basins. Many scheduling schemes -- with varying levels of theoretical justification and/or empirical vetting, but no definitive evidence across all possible application scenarios -- have been proposed, and their actual choice often depends on habit, compromise between improvement and additional hyperparameters to tune, or \textit{brute force} \textit{trial and error}.

\subsubsection{\textit{Algorithmic differentiation} and the \textit{backpropagation} algorithm}

We are left -- at this point -- with one last, but crucial, question: how does all the \textit{machinery} outlined until now work in practice, at the implementation level? In fact, until now, we have neither talked about how the terms $\vec{g_i}$ in (\ref{eq:1}) are computed from the mere datapoint-wise knowledge of $\mathcal{L}_{\mathcal{N}}(\vec{x}|\vec{\theta})$, nor we have put specific constraints on the differentiability (or even continuity!) of $\mathcal{L}_{\mathcal{N}}$: and in fact such constraints are mostly unnecessary in practice\footnote{Indeed, heuristically -- for any reasonably modern \textit{automatic differentiation} framework, such as \href{https://pytorch.org/}{PyTorch}, see \cite{PaszkeEtAl2019PyTorch} -- it is strictly sufficient that $\mathcal{L}(\vec{\theta})$ and $\mathcal{L}(\vec{x})$ are piecewise \textit{dual-number}-differentiable within open sets, and such \textit{pieces} definable by algorithmic branching and/or recursion. \textit{I.e.} discontinuous activation functions are allowed, and so are selection function such as in-place sorting.}.

The core ingredient seamlessly allowing such kind of computations is (collectively) called \textit{automatic} or \textit{algorithmic} \textit{differentiation} -- and refers to a general algorithmic strategy to determine exact pointwise evaluations\footnote{\textit{I.e.} without resorting to \textit{numeric approximation} of derivatives, and neither computing them symbolically such as in \textit{computer algebra systems} or \textit{e.g.} \href{https://www.sympy.org/}{\texttt{SymPy}}.} of derivatives for (practically) any piece of \textit{legal} code in a given programming language. The underlying principles and technicalities powering such approaches go far beyond the scope of this work. As a \textit{proof-of-concept} justification, an intuitive mechanism allowing similar flexibility (though rarely used in modern \textit{AutoDiff} frameworks) -- that of \textit{dual numbers} -- is thoroughly described in \cite{Fischer1998DualNumbers}. An accurate and comprehensive survey of \textit{automatic differentiation} methods for machine (and \textit{deep}, indeed) learning is available in \cite{BaydinEtAl2018AutoDiff}.

Finally, in order to ensure computational efficiency for the whole process, the \textit{error backpropagation} algorithm is employed -- guaranteeing that the term $\vec{g_i}$ is computed linearly in the number of parameters \wrt the number of atomic differentiation operations. This comes possible thanks to the graph-based representation of a neural architecture, the application of the \textit{chain rule of differentiation} (by noting that in a \textit{multilayer} architecture the derivatives of representations at layer $i$ only depend from at most all the representations of layer $i-1$), and the \textit{memoisation} of such derivatives during the computations required by those of the innermost layer. Such approach to the computation of the entire $\frac{\partial\mathcal{L}(\vec{\theta})}{\partial\vec{\theta}}$ Jacobian from a reverse network-graph traversal (and the relevant related data structures built upon it) is called \textit{reverse-mode automatic differentiation} and powers the large majority of modern deep learning libraries.

\subsubsection{\textit{(more)} Advanced topics in \textit{deep learning}}

To conclude the overview of preliminary knowledge used in the portion of work that follows, this section will present some -- selected -- fundamental principles and results taken from deep learning theory or practice, with no goal of completeness. Such abridged exposition -- at the cost of partiality -- represents though the collection of core underpinnings on which our novel proposal is based, as opposed to the easy misconception of it being just a \textit{random sample} of tricks and fortuitous consequences.

\paragraphnl{\textit{Deep Learning} as \textit{homoiconic} approximate probabilistic programming}
Even though the previous sections may have given the impression of deep learning as a field composed of mostly disjoint techniques and \textit{architectures}, remarkable synergy among all different \textit{moving parts} is one of its most distinguishing elements. To the point that its development and use -- a concept spearheaded since 2018 by one of its pioneers, Yann LeCun -- can even be viewed through a \textit{fresh} lens as a mostly compositional activity, in a similar spirit as organic synthesis or computer programming. Indeed, such latter analogy -- that of a set of pre-constituted patterns (\textit{e.g.} constructs such as \textit{declarations of variables}, \textit{typing}, \textit{conditional statements}, structuring in \textit{functions} or \textit{objects}, use of design patterns, \textit{...}) each with self-standing dignity and theoretical justification, but variously and cleverly extended, assembled, and vetted as a new whole -- is also a remarkably clear high-level description of the \textit{true nature} of modern deep learning: the composition of transformations between real-valued, (usually high-dimensional) vector spaces -- whose very nature is learnt through optimisation of an adequate metric -- of which the \textit{entry-} and \textit{exit-} \textit{points} may be the actual training or inference data, but whose internal representations and dominant ingredients are \textit{probability distributions} or surrogates thereof.

This description, together with the realisation that, indeed, such \textit{learnably-parametrised} transformations may be able to naturally \textit{end-to-end} transform  inputs into outputs in ultimately an approximately same way \textit{traditional} computer programs do\footnote{And even \textit{Turing-completely} so; see \textit{e.g.} \cite{PerezEtAl2018Turing}.}, allows to properly frame deep learning as an adaptive platform for example-driven programming that is both \textit{homoiconic}\footnote{\textit{I.e.} such that the \textit{program} is at the same time represented in terms of (some of) its \textit{primitive types} -- either \textit{parameters} or \textit{representations} in the case of deep learning, when not even both.} and able to express uncertainty probabilistically, yet within a finite, well-defined description.

Additionally, such viewpoint leads remarkably close to \textit{neural computation} in biological systems, where a similar framing of the field is offered in terms of (biological) neurons capable of plastic analogue processing of frequency-coded electrical stimuli (with the astounding and tangible result that cognition, indeed, is!) -- further strengthening the connections\footnote{Pun involuntarily intended.} of deep learning with its original motives.

\paragraphnl{\textit{Auto-encoders}: compressing by \textit{learning to reconstruct}}
Back to a more specific description, the first of such aforementioned \textit{patterns} we will describe is that of the \textit{auto-encoder}\footnote{A nowadays well-established architecture in the field, for which it is difficult to backtrace to a clear \textit{first} in literature. See, \textit{e.g.}, \cite{Kramer1991Autoencoders}.}: a peculiar deep learning architecture used to first learn, and then apply, a \textit{compression} of input data. In this case -- and almost always, unless explicitly specified -- such input is to be considered part of a somehow structured collection: being it that of the samples of a specific -- but potentially unknown -- probability distribution of interest (or a mixture of them), or more broadly that of inputs generically satisfying a property (\textit{e.g.} being natural images -- \textit{i.e.} not being purposefully generated in a corner-case manner, for whatever reason! --, being recordings of a human voice or animal call, being time \textit{ticks} of stock title pricing, \textit{etc...}), often enough to specify the context dictating their use.

And indeed, the goal of finding any \textit{compressed representation} for the data of interest directly translates in exploiting the properties of the overall collection -- or the collection of properties of individual datapoints -- to reduce their dimensionality in the most information-preserving\footnote{Here intended both in the \textit{information-theoretical} sense, more formally -- but also in the less \textit{well posed} meaning of \textit{"being able to preserve all features of interest, while potentially discarding the less relevant ones"}.} fashion.

The practical approach required to accomplish such (potentially very hard!) goal is on the other hand particularly simple.

The neural architecture will be composed of two subnetworks $\netw{E}$ and $\netw{D}$ -- respectively the \textit{encoder} and the \textit{decoder} -- linked by the following conventional relations:
$$\vec{c} = \netw{E}(\vec{x}); \
\vec{\tilde{x}} = \netw{D}(\vec{c})$$
with, usually, $\dimof{\vec{c}} \ll \dimof{\vec{x}}$ ad a loss encoding the similarity between the original input $\vec{x}$ and $\vec{\tilde{x}}$, that constitutes in this case its tentative reconstruction -- \textit{e.g.} an $L_2$ similarity metric \suchthat $\mathcal{L}_{\text{AE}}(\vec{x}) = \normof{\vec{x} - \netw{D}(\netw{E}(\vec{x}))}_2$.

The \textit{dimensionality bottleneck} -- also called indeed an \textit{information bottleneck} -- induced by the property $\dimof{\vec{c}} \ll \dimof{\vec{x}}$ forces the network, trained under the loss-driven optimisation, to reconstruct the original input from a much smaller-dimensional representation of it; and as a consequence, forces the \textit{encoder} to convey as much information as possibly learnable in the $\vec{c} = \netw{E}(\vec{x})$ step -- with the \textit{code} $\vec{c}$ being the compressed result sought after.

At this point -- with $\netw{D}$ usually discarded except in very particular scenarios -- the learnt \textit{compressor} $\netw{E}$ may be used on any potential input, in order to obtain a \textit{compressed representation} of it -- which is then treated as its characteristic, lower-dimensional, set of \textit{features} for further processing.

One potential downside of such architecture -- beyond the specific task of learning a \textit{compression} of the data -- is the fundamental lack of use for the corresponding \textit{generator}, which is nonetheless functional for the whole, but not on its own.

\paragraphnl{Learning distributions by \textit{auto-encoding} sampled data}
Right from such latter observation, one can consider the thought process leading to the development of the \textit{variational}\footnote{See \cite{KingmaWelling2014AutoEncoding} for the paper in which it was introduced.} and \textit{variational conditional}\footnote{See \cite{SohnEtAl2015CVAE} for the paper in which it was introduced.} autoencoders: probabilistic counterparts of the \textit{simple autoencoder} which allow generative modelling (and indeed whose \textit{encoder} alone is almost never used on its own).

The main issue in dealing with the \textit{decoder} of an \textit{autoencoder} in the attempt to generate samples from the same collection -- or more precisely, in this case a proper \textit{probability distribution} of the inputs --  is the \textit{understructuredness} of the \textit{latent space} (\textit{i.e.} the space of the codes). In fact -- being autoencoders only trained to match single given inputs to single learned codes (and viceversa) -- no guarantee is offered on the learnability of a \textit{meaning} for codes outside those coincidentally correspondent to inputs \textit{fed} at inference time. Only in such case, a well-formed reconstruction is probabilistically guaranteed within arbitrarily tight bounds\footnote{In the sense of \textit{Probably Approximately Correct} bounds, given enough -- but finite -- width and depth of the architecture, training data, training epochs, and a sufficiently large dimension for the codes. For a hint on how such actual estimations are performed, see \textit{e.g.} \cite{EpsteinMeir2019AEPAC} and \cite{ZehaoEtAl2020AEPAC}.}.

Additionally, even in the case a \textit{probabilistic} device is employed in order to \textit{inform} the encoder/decoder pair about a sampling process happening in latent space (as will indeed be the case for the \textit{variational} class of autoencoders), the tractability of the sampled posterior (\textit{i.e.} the output of the decoder, in this setting) is far from guaranteed \wrt differentiation and thus loss gradient computation -- both in general terms due to the sampling itself, whose parameters may be inter-dependent, and for specific, but arbitrary, probability distributions to be sampled from in latent space.

Both these concerns are addressed -- for parameter-only conditional \textit{p.d.f.}s (e.g. in the form $\vec{x} \sim \vec{p}(\vec{\tilde{x}} \condon \phi_1, \dots, \phi_{s\in\mathbb{N}})$) by maintaining the same architecture for the encoder/decoder pair, further endowing it with:
\begin{itemize}
    \item An actual \textbf{sampling in \textit{code-space}}: the result of the encoding is not a deterministic code, but a collection of ordered parameter-vectors specifying a given-form \textit{p.d.f.}, and a sample of whose is passed onto the decoder.
    \item A latent \textit{p.d.f.} adequately reparametrised (\textbf{\textit{reparametrisation trick}}) in such way that the model posterior may be obtained as $\vec{\tilde{x}} = \netw{D}_{\vec{\theta_{\mathcal{D}}}}(\vec{c})$ -- a deterministic function of $\vec{c}$, parametrised by the weights of the decoder -- and $\vec{c} \sim \vec{p}_{\text{latent}}(\vec{c}\condon \netw{E}_{\vec{\theta_{\mathcal{E}}}}(\vec{x}))$.
\end{itemize}

Since $\vec{p}_{\text{latent}}$ cannot be jointly learnt from data and being able to satisfy the requirements of the \textit{reparametrisation} trick, the loss function to be adopted should both ensure that $\vec{x}$-$\vec{\tilde{x}}$ similarity is preserved, and that the samples $\vec{c} \sim \vec{p}_{\text{latent}}(\vec{c}\condon \netw{E}_{\vec{\theta_{\mathcal{E}}}}(\vec{x}))$ are close to those of a known, given \textit{p.d.f.} from which they will be sampled at inference time to observe new posterior samples as the output of the decoder.

The resulting loss becomes $\mathcal{L}_{\text{VAE}} \coloneqq \mathcal{L}_{\text{AE}} + \kldiv(\vec{p}_{\text{latent}},\vec{p}_{\text{lobs}})$, with $\vec{p}_{\text{latent}}$ the desired latent \textit{p.d.f.}, $\vec{p}_{\text{lobs}}$ the observed latent \textit{p.d.f.} obtained as $\vec{p}_{\text{latent}}(\vec{c}\condon \netw{E}_{\vec{\theta_{\mathcal{E}}}}(\vec{x}))$, and $\kldiv$ their \textit{Kullback-Leibler divergence}.

Overall, the process described by the minimisation of such \textit{loss}, closely resembles that of \textit{variational free energy minimisation}, whence the name of such architectural \textit{pattern}.

Finally, in order to sample from a conditional \textit{p.d.f.} the \textit{random variate} posterior\linebreak
${\vec{\tilde{x}_{\text{r.v.}}} \sim \vec{p}(\vec{\tilde{x}_{\text{r.v.}}} \condon \vec{{x}_{\text{c.v.}}}, \phi_1, \dots, \phi_{s\in\mathbb{N}})}$, given a set of conditioning variables $\vec{{x}_{\text{c.v.}}}$, the training protocol is further modified. Both $\vec{{x}_{\text{r.v.}}}$ and $\vec{{x}_{\text{c.v.}}}$ are jointly given as input to the encoder, and the \textit{code} is built by further concatenating the output of the encoder with $\vec{{x}_{\text{c.v.}}}$. The loss function and the generative process are the same as in the case of \textit{VAE}s.

\subsection{\textit{Adversarial Robustness} (and lack thereof)}

The previous sections -- though tentatively \textit{hype-agnostic}, but also never denying evidence of success! --, by describing only what we know \textit{has worked} so far, may have given the impression of \textit{deep learning} as a flawless paradigm able to solve any problem \textit{thrown at it}, no matter how tractable or difficult to think it as such\footnote{It is difficult not to think at the fact that a similar trend, is happening -- this time \textit{for real} (or maybe \href{https://cutt.ly/europe_ai_startup_not_ai}{even not so \textit{real}}) -- in the small/medium-sized industry landscape, trying to deal with \textit{artificial intelligence}.}.

In spite of extraordinary results -- at even \textit{human} or \textit{better-than-human} \textit{level} in a well-specified set of tasks -- \textit{deep learning} is not free from pitfalls. Among which, that of \textit{vulnerability to adversarial attacks}, which constitutes a major hurdle to the adoption of deep learning in safety-critical or heavily-regulated scenarios, and among the most relevant to address in order for the public to trust it with their most valuable assets or decisions. Additionally, the study and development of methods to improve \textit{adversarial robustness} have also a direct effect in enhancing the behaviour of solutions in presence of \textit{non-adversarial} but \textit{unforeseen}, \textit{corner-case}\footnote{As we will see, the phenomenon of \textit{adversarial attacks} rises indeed from -- quite literally -- \textit{corner-cases}, in classification settings.} inputs.

\subsubsection{\textit{Essential} definition(s)}
Trying to preserve the greatest generality possible, \textit{adversarial attacks} towards a machine learning system are specially-crafted (inference-time) inputs designed to disrupt its expected behaviour. As most often referred to in the context of classification -- which we will discuss in this thesis -- we will focus on inputs to a classifier, able to produce a \textit{misclassification}.\footnote{Let us preliminarily address a typical concern, here. Indeed one may ask what \textit{exactly} a \textit{misclassification} or \textit{unexpected} behaviour is. Without delving into an epistemological \textit{rabbit hole}, it will suffice to define them as any marked difformity \wrt the (rational, informed -- if applicable) confident decision of the prototypical system the model is trying to mimic. \textit{E.g.} in the case of image captioning for a \textit{social network} platform, any caption grossly misdescribing the original picture for the majority (or \textit{key interest groups}) of its users (or a statistical sample thereof).}

\paragraphnl{Commented systematics of \textit{adversarial attacks}}
Still within the scope of (supervised) classification, we can first classify \textit{adversarial attacks} \wrt the types of anomaly they seek to produce in the attacked model.

Focusing on the \textbf{input}, we have:
\begin{itemize}
    \item \textit{(purely)} \textit{Synthetic} attacks, if they produce inputs exclusively from the use of the attacked model -- and, of course, of an \textit{attack generation algorithm} -- (\textit{e.g.} excluding the possibility of exploiting external knowledge, datasets, \etc). The result is usually similar to data \textit{noise}, generally carries no resemblance with elements of a hypothetical training dataset, and (in cases where inputs are perceivable) is easily recognised as anomalous by a trained human operator.
    \item \textit{Perturbative} attacks, if they are expressed as a \textit{perturbation} of legitimate -- already known to the attacker -- input. Perturbations are usually additive and constrained in their norm\footnote{In the scenario -- assumable \wlogg -- where inputs are real-valued vectors or mappable as such.}, resulting in realistic and often undetectable (provided the constraint is tight enough) perceptual alterations with high effectiveness in \textit{fooling} the classifier. They usually carry the highest risk, due to their scarce detectability (even by machine learning models themselves trained for the task -- being them susceptible to the same kind of attack!)\footnote{Additionally, \textit{perturbative} attacks include -- for a constraint lax enough, and starting from whichever single one input -- all possible \textit{purely synthetic} counterparts. This makes the \textit{perturbative} setting a sufficient one to be studied.}.
\end{itemize}

If we move to the analysis of \textbf{outputs} (or better, the changes in output before and after the attack)\footnote{We implicitly assume here, and in all the following, the perspective of \textit{top-1 accuracy under attack assessment}: the model is considered as an \textit{end-to-end} system outputting the most probable class corresponding to its input -- the only class we care about. This has numerous advantages \wrt the work that follows, as it allows for easier comparisons of attacks exploiting different vulnerabilities in the attacked classifier, is generally harder to recover \wrt the unattacked model, and avoids the otherwise \textit{cyclical reference} fallacious rank-based approaches, implicitly relying on the fact that the positions of output classes in a ranking are conditional on the previous ones having \textit{truly} that position -- which is disproved after a successful \textit{top-1 class attack}.}, we can distinguish among:
\begin{itemize}
    \item \textit{Targeted} attacks, if the result of the attack should be classified as a given, pre-stated, class. Or, more rarely, if the input to be perturbed should belong to a given, pre-stated, class.
    \item \textit{Untargeted} attacks, if a change in output is the only desired outcome -- with no further constraint. Generally speaking, \textit{untargeted} attacks are easier to perform successfully -- and more difficult to defend against. Additionally, the set of \textit{untargeted} attacks also contains the \textit{best}\footnote{In the sense of \textit{lower loss $\argmin$}; this will be clear after we discussed how (the greatest majority of) attacks are generated.} \textit{targeted} attack among all classes.
\end{itemize}

Finally, we can further subdivide \textit{attacks} in relation to the \textit{\textbf{degree of knowledge}} of the \textit{adversary} about the model attacked. We can perform:
\begin{itemize}
    \item \textit{Black-box} attacks -- \textit{i.e.} attacks relying only on knowledge extractable from the model along strictly expected use. This includes, and is restricted to, inputting data and reading the corresponding output. No further access is given to model architecture, structure or representations/gradients. This is the less susceptible case and corresponds to the model being deployed behind an \textit{ideal} API (with no rate limiting, though), and maximally relying on \textit{security by obfuscation}.
    \item \textit{White-box} attacks -- \textit{i.e.} attacks that can rely on all extractable knowledge from the model, as if \texttt{root} access was granted to the very system the model is running on. This includes the ability to input data into the model, read the corresponding output, but also its architecture, implementation details, representations, gradients, auxiliary variables of layers (\textit{i.e.} \textit{batch normalisation} statistics) or of the optimiser. The threat model closely reflects that of services operating on a compromised system, or that of commercial items that can be bought and probed in a protected environment. This is the hardest scenario for the defender -- and reasonably the most realistic for the widest array of \textit{actually} deployed products.
    \item \textit{Grey-box} attacks -- comprising all attacks lying in between the two: usually a \textit{white-box} with additional restrictions imposed.
\end{itemize}

\subsubsection[{A geometric intuition: the \textit{manifold hypothesis}}]{{A geometric intuition: the \textit{manifold hypothesis}}\protect\footnote{For a much more \textit{in-depth} analysis, see \textit{e.g.} the excellent \cite{Dube2018HighDS}.}}

In order to better reason about \textit{adversarial attacks}, \textit{defences}, and how it is possible to improve the \textit{robustness} of deep neural models -- let us focus first on a (relatively intuitive\footnote{By choice, and not being a necessary requirement for the theory itself -- whose exhaustive and far-reaching analysis is out of the scope of this work.}) introduction to the \textit{data (sub)manifold hypothesis}: it will also naturally lead to a precise description of \textit{perturbative} adversarial attacks, and some quantitative measures for their assessment.

In the general setting of a classifier accepting \textit{real vector-valued} inputs of given dimension, the model itself will produce a valid class as output, among those on which it has been trained\footnote{This explicitly excludes classifiers specifically-built for \textit{open set detection}. However -- even thought the inner dynamics leading to an input being classified as part of the \textit{open set} are different from those class-specific -- the \textit{open set} may be considered just as an additional class like all the others, and the reasoning will still be valid!}, independently from the actual nature of such input (\textit{e.g.} it being a \textit{natural image} or not) and just provided it is \textit{legal}. This allows for a hypothetical, idealised, exhaustive labelling of any possible \textit{legal} input with the class in which it is mapped through the classifier -- which is indeed a \textit{partition} and equivalent to a full specification of the trained model. We will now ask how \textit{input data of interest} are \textit{placed} within such partition and in relation to their ambient space.

The \textit{data manifold hypothesis}\footnote{Even though called a \textit{hypothesis}, of such next claim there are theoretical supporting arguments, and experimental proofs within specific data domains -- even dating back thirty years, though not directly in this form; see \cite{Laughlin1994FlyEye} for an interesting example of such kind. What it is more \textit{hypothetical}, indeed, is its relationship with \textit{robustness}.} posits that data belonging to the hypothetical collection on which the classifier is \textit{properly} expected to be used -- and of which, excluding \textit{gross} mistakes or procedures explicitly meant to be otherwise, also the \textit{training data} is part -- belong to a manifold whose intrinsic dimension is (much) smaller \wrt its ambient space. On such manifold, we could\footnote{The hypothetical is used because of the impossibility of doing it exhaustively, unless the dataset is algorithmically generated with such goal in mind.} also draw the \textit{ground truth} decision boundaries we would expect the \textit{ideal} classifier to abide to.

From the intersections of the \textit{data manifold}, \textit{ideal decision boundaries} on it, and classifier \textit{decision boundaries} in input space, we can identify the following regions (in input space):

\begin{itemize}
    \item The intersection of \textit{ideal} and \textit{actual} decision regions, on the \textit{data manifold}. In this region the learned classifier behaves exactly as expected \wrt the predicted class.
    \item Still on the \textit{manifold} -- outside the previous region -- and close to \textit{misplaced} decision boundaries of the classifier. Even though such region is the \textit{richest} of training input data, this is very often not enough to ensure exactly \textit{ideal behaviour} is observed\footnote{And indeed, the behaviour of the classifier is very likely still \textit{extrapolatory}: see \cite{BalestrieroEtAl2021Extrapolation}.}. Inputs belonging to this region will definitely \textit{look} real (or an interpolation of different realistic datapoints): some of them will surely be \textit{perturbative adversarial attacks} to the classifier.
    \item Still on the \textit{manifold} -- outside previous regions -- and far from the regions of high \textit{boundary-point density}. Training datapoints in such region should be minimal (indeed, exactly zero in the case of an \textit{ideally-regularised} model of the adequate size and trained until complete convergence) and misclassifications occur due to \textit{e.g.} the represented class being not a \textit{legitimate} output of the classifier\footnote{As an extreme example, a classifier perfectly trained to assign the animal species to photos of animals will still classify \textit{non-animals} as a particular species of them, by visual similarity or -- most often -- due to the effects of confounders.}. Such region does not contain (practically \textit{by convention}) \textit{adversarial attacks} and poses little risks for the user or the provider, and are easily recognisable from experience.
    \item The \textit{off-manifold} region -- for which a \textit{true class} is not even definable. Such region contains the most \textit{adversarial attacks}, including all those resulting from larger perturbations. It is practically unavoidable for any classifier trained on \textit{clean}\footnote{In the sense of \textit{natural}, \textit{non-adversarial} examples.} inputs of sufficiently high dimension\footnote{See, \textit{e.g.} \cite{BortolussiSanguinetti2018IntrinsicGV}.} and poses manageability problems for such reason and due to its lack of structure \wrt the \textit{data manifold}.
\end{itemize}

In the usual process of \textit{attack generation} -- outlined below -- none of such region is \textit{intentionally} targeted (with minor exceptions pertaining only to the \textit{off-manifold} region), but one of them necessarily results as containing the best solution of a \textit{loss-minimisation} problem similar (if not identical, in some cases) to that of \textit{training} an artificial neural network.

\subsubsection{\textit{How it's made:} Attacks}
From the perspective of an \textit{attacker}, no attack is better than that which succeeds. While such truism may justify \textit{literally} any technique resulting in even occasional misclassifications -- which, even if the result of sheer luck, may carry extraordinary amounts of risk or actual damage, nonetheless --, a more systematic approach to the problem, especially from the standpoint of the \textit{defender} who must counteract such attacks, strongly benefits from mathematical formalisation and amenability to optimisation strategies to automate, speed up, and pinpoint the search of relevant perturbations.

Given the generality of the \textit{perturbative} setting, attacks usually seek additive perturbations to knowingly \textit{legitimate} inputs, bound by a measure of strength.\footnote{Which may be given -- as anticipated -- \textit{e.g.} by the norm of the perturbation. If such constraint is not into place, one may think of perturbations given by the difference of two legitimate input points: the resulting \textit{perturbed input} is a \textit{successful attack} by constructive definition, yet definitely not the intended goal.}. An \textit{$\epsilon$-perturbative} adversarial attack to the classifier $\netw{N}$ (considered as an \textit{end-to-end class-outputting device}), given $\epsilon \in \mathbb{R}_{+,\setminus0}$ and the norm $\normof{\cdot}$, is thus any point $\vec{x^\star} = \vec{x_{0} + \vec{p}}$ \suchthat $\netw{N}(\vec{x^\star}) \neq \netw{N}(\vec{x_0})$\footnote{This is indeed the \textit{untargeted} version of an attack. The \textit{targeted} equivalent is obtained by replacing the inequality with an equality to the \textit{target class}, different from the original. The analysis will focus on the former, with the latter always easily obtained with minimal modifications therein.} and $\normof{\vec{p}} < \epsilon$ for some given $\vec{p}$, and \textit{legitimate} input $\vec{x}$.

The problem can now be decomposed as follows. For each among some $\epsilon$-balls centred at \textit{knowingly unperturbed} datapoints (within the input space, and defined by $\normof{\cdot}$), the optimal $\vec{p}$ is determined according to some \textit{loss} quantifying the confidence in the success of the resulting \textit{perturbed input} if used as an attack.

\paragraphnl{\textit{White-box} scenario}
In the \textit{white-box} setting, the knowledge of gradients within the model generated by any input conceivable allows for \textit{gradient-based} optimisation schemes, \textit{mutatis mutandis} similar to those described for the tentatively-optimal choice of weights during training. In particular, the weights are kept fixed while the optimal perturbation (or, directly, \textit{adversarial input}) is optimised \wrt to the uphill direction of the gradient induced by a \textit{similarity loss} not in principle different from that used to train the network.

Of this kind, \textit{e.g.} and absolutely not exhaustively, but of particular interest:
\begin{itemize}
    \item The \textit{FGSM attack}\footnote{Introduced in \cite{Goodfellow2015Harnessing}.} (\textit{i.e.} \textit{Fast Gradient Sign Method}), which iterates just once along the uphill gradient sign direction, with a \textit{displacement} in input space of norm $\epsilon$.
    \item The \textit{PGD attack}\footnote{Introduced in \cite{MadryEtAl2018Towards}.} (\textit{i.e.} \textit{Projected Gradient Descent}, reasonably to be considered an iterative extension of the \textit{FGSM}. In particular, iterations of variable norm (not necessarily $\epsilon$) are performed in the direction of the local gradient and proportionally to its modulus -- starting from a \textit{legitimate input} and for a given number of times; after each step, if the resulting point falls outside the $\epsilon$-ball of interest, it is orthogonally reprojected on its border.
\end{itemize}

Of a different kind, \textit{e.g.}, the \textit{DeepFool attack}\footnote{Introduced in \cite{MoosaviDezfooli2016DeepFool}.}, instead, explicitly looking for the closest \textit{perturbed inputs} at the intersection of the \textit{input space} with the hyperspace orthogonal to the binary\footnote{In the case of \textit{untargeted} attacks; additional care should be exercised in case of \textit{targeted} ones.} \textit{one-vs-all} boundary of the input class, still within the given norm constraint.

\paragraphnl{\textit{Black-box} scenario}
In the case where \textit{model gradient} information is missing or unavailable, the success rate of attacks is reduced \wrt their \textit{white-box} counterparts. However, still effective techniques rely on:
\begin{itemize}
     \item \textit{Gradient-free} optimisation schemes (\textit{e.g.} genetic programming, \textit{reinforcement learning} approaches) with actual success rate (or any \textit{proxy} metric) as the function to be maximised;
     \item Machine learning systems aimed at directly sampling from an \textit{adversarial region} of the input space, whose identification is delegated to an additional model to be trained on similar (or the exact, if available) inputs to those employed for the attacked model. Of this kind, \textit{e.g.}, \textit{GAN-based}\footnote{Where two deep neural networks learn each to perform a different task: one must map random-noise samples into \textit{realistic-looking} images, while the other must discriminate whether an input is sampled from an unperturbed training set or produced by the other network. In a turn-based, \textit{minimax} game, the \textit{generator} becomes incrementally better at its task, under the improved capabilities of the, improving too, \textit{discriminator}. In the use of similar schemes for \textit{adversarial attack generation}, information about the class put out by the \textit{target classifier} should also be included somehow in the loss of the generator. For an \textit{in-depth} overview of \textit{GAN}s, see \cite{GoodfellowEtAl2014GANs}.} attacks -- which can even be used in a \textit{white-box} setting, by variously providing the additional information to one or either the two networks involved in \textit{GAN} training (of this type -- both \textit{black} or \textit{white}\textit{-box} -- \textit{e.g.} \texttt{AdvGan}\footnote{See \cite{XiaoEtAl2018Generating}.}).
     \item \textit{White-box} attacks targeting surrogate models (or mixtures thereof) of the one to be attacked. Such surrogate models may be obtained by \textit{re-training} them on the same (or similar) dataset used for the model actually under attack\footnote{Of this type, \textit{e.g.} the \href{https://cutt.ly/tramer_mnist_pgd_on_copies}{solution by Florian Tramèr to Aleksander Madry's MNIST Adversarial Examples Challenge} for the \textit{black-box} setting, 2017 -- still the second-best even after 5 years of open leaderboard.} -- or directly (and expensively) derived just from the \textit{input/output} pairings produced by it.
\end{itemize}

\subsubsection{\textit{How it's made:} Defences}
While research of \textit{attacks} is fundamental in advancing the field -- with better understanding of the actual dynamics determining such vulnerabilities, and to promote \textit{offensive security} of machine learning systems -- the \textit{holy grail} of \textit{adversarial robustness} research is to produce models, training algorithms, and inference protocols able to counteract such weakness, for the development of more trustworthy \textit{AI}, and \textit{for the greater good}.

Equally extensive, but often relying on much deeper knowledge and elaborate procedures \wrt that of \textit{attacks}, the field of \textit{adversarial defences} is in continuous evolution and rooted on an always renewing body of intuitive, empirical, and formal knowledge.

The main avenues of successful, practical developments in the field are the following:

\begin{itemize}
    \item \textit{\underline{Adversarial training}}\footnote{Originally proposed as a technique in \cite{Goodfellow2015Harnessing}.}. By large margin the most used approach -- consisting in augmenting the training set of a model (of given weights, potentially at any point along its training) with \textit{adversarial inputs} generated according to specific attacks (\textit{e.g.} from the set right discussed) but endowed with their original label, and continue training on the augmented dataset. Expanding the \textit{on-manifold} regions where the classifier works as expected with ever new \textit{perturbed} datapoints where they are most needed, and providing a \textit{true label} for selected points \textit{off-manifold}, it is one among the few techniques with full applicability to any already existing architecture. This comes at the cost of the necessity for \textit{model/dataset-specific} re-training, and much increased reliance on the correct \textit{threat modelling} choices to mirror the (even \textit{unforeseeable}) threats to be faced -- though transferability of defences is partially possible across models, datasets, and attack types/strength.\newline
    In such regard, specific attacks are of particular interest -- so-called \textit{universal}: those potentially able to converge within any point of the $\epsilon$-ball of choice (as opposed to those operating only at a fixed displacement from the original input, or showing preferential directions), and thus able to theoretically produce the maximally representative \textit{perturbations} for the model to learn from within adversarial training. \textit{E.g.} the \textit{PGD attack} is \textit{universal}; \textit{FGSM} it is not, due to the fixed displacement of exactly $\epsilon$.\newline
    Importantly, \textit{adversarially-trained} models can still be attacked with the same techniques (indeed just their weights have changed, not the threat model): the defence is meant to reduce their effectiveness, and on average increases the time or number of iterations required to find a successful perturbation.\newline
    Finally, multiple \textit{adversarial attacks} (in the sense of different types, strength, even threat model) may be used within the adversarial training of the same network. This process -- though effective -- comes to terms with what is indeed a training on a much larger dataset of examples, sometimes carrying conflicting perturbations \wrt to the optimal way to update model parameters.\footnote{An experimental evaluation of the process -- and much more! -- is given \textit{e.g.} in \cite{TramerBoneh2019Adversarial}.}
    \item \textit{\underline{Adversarial detection}}. Framed as the solution of a binary classification problem, this approach aims at just \textit{detecting} whether (or with what probability) an input is \textit{adversarial} \wrt a given model, inviting the user to caution (or rejecting the output right after) in case it probably is. Typical approaches tailored toward \textit{anomaly detection} may be employed, with very little domain-specific adaptation. \textit{E.g.} the \textit{discriminator} of a \textit{GAN} of the type described in previous-page footnote can be used for the purpose.\newline
    Though a potentially interesting track to pursue, such techniques do not attempt to \textit{solve} the problem: they seek to make it manageable. Indeed, they are inherently susceptible of \textit{denial-of-service} attacks -- by adversarially targetting the \textit{detector} to always raise a flag, extending \textit{attack surface} of the system, and carrying no guarantee of success.
    \item \textit{\underline{Adversarial purification}}. Similar in goals to \textit{adversarial training} (\textit{i.e.} directly attempting to ensure a correct classification of \textit{adversarial attacks}) and in tools to \textit{adversarial detection} (\textit{i.e.} delegating the task to a different system than the attacked), such class of methods aims at \textit{recovering} from the eventually \textit{perturbed} input its original, \textit{clean} version -- by the development of a unified mapping of \textit{clean} inputs into themselves, and of \textit{adversarial} ones back across the additive perturbation. Such technique, similar in framing to more typical \textit{denoising} or \textit{controlled editing} tasks, may be performed in a fashion completely independent from the \textit{true labels} of data, thus being usually free from the burden of sometimes required additional labelling efforts. Supervised, self-supervised (even \textit{energy-based}) and weakly-supervised alternatives in the same spirit do further exist. \textit{Encoder/decoder} architectures, and conditional generative models are dominant within this class of approaches.\newline
    Due to the lack of specificity to the attack, or reliance on \textit{true labels}, these defences can become very creative. As illustrative examples:\newline
    $\longrightarrow\ $\texttt{PuVAE}\footnote{See \cite{HwangEtAl2019PuVAE}.} obtains an increased robustness to \textit{attacks}, without the need for \textit{adversarial training} by first learning a \textit{true class}-\textit{conditional VAE} to map an adversarially-perturbed input and its \textit{true class} to its purified version. Then, at inference time, the pair of \textit{adversarial input / each class} is passed through the model and the resulting reconstruction bearing the most similarity with the perturbed image, is selected and finally classified.
    \newline
    $\longrightarrow\ $The generator of \texttt{Defense-GAN}\footnote{Proposed in \cite{SamangoueiEtAl2018DefenseGAN}.}, instead, learns to generate tentative input reconstructions -- subject to difference-norm constraints \wrt the perturbed input -- while the discriminator learns to score the \textit{adversarial likelihood} of the result. Still being one of the best-performing defences in both \textit{white-box} and \textit{black-box} scenarios, such device carries the cost of extremely lengthy training times, and much \textit{longer than average} inference times too.
    \item \textit{\underline{Inference-time defences}}. Class of techniques containing various approaches trying to enhance resilience to \textit{adversarial attacks} by adapting the inference protocol of the model. Inspired by \textit{test-time augmentation} and still in their relative infancy, the most remarkable contributions propose to classify copies of the same input subject to standardised transformations (\textit{e.g.} in the case of images: rotation, cropping, \etc) -- provided that the model has been trained to correctly classify analogously-transformed \textit{clean} inputs. Such approach usually -- but inconsistently -- decrease the success rate of adversarial attacks, unless the \textit{adversary} succeeds in the much harder  goal of producing correspondingly invariant attacks.
    \item \textit{\underline{By leveraging a theoretically-robust structure}}. Defences within this class -- though even extremely different among themselves -- all try to transpose into usable models some theoretical insights coming from an experimental or conjectural study of knowingly robust models \textit{components}. A comprehensive theory of robustness is still lacking; nonetheless some results of such kind have been successfully obtained. Examples may include \textit{Parseval Networks}\footnote{See \cite{CisseEtAl2017Parseval}.}, trying to link well established mathematical properties of the function described by a model (\textit{e.g.} its Lipschitz constant) with its robustness -- and inform consequently model architecture design; or \textit{\texttt{kWTA} networks}\footnote{See the already-mentioned \cite{Xiao2020Enhancing}.}, in which neuron-wise activation functions are replaced with layer-wise equivalents based on sorting functions and much more robust after a successful adversarial training (a result not to be taken for granted, in comparison with \textit{traditional} identical counterparts).
    \item \textit{\underline{By changing the rules of the game}}. A whole topic of its own, proposing completely different architectures for models or their parts (\textit{e.g.} based on \textit{spiking neural network} models), or different training algorithms, based on different interpretations of the network parameters or representations. Of this latter kind, \textit{Bayesian Neural Networks} -- whose robustness against \textit{gradient-based} attacks has been well established by \textit{e.g.} \cite{CarboneEtAl2020Robustness}, at the cost of much higher training complexity.
\end{itemize}

\subsubsection{On threat models, attacks/defences transferability, unforeseeability}
From the outline just described, it is clear that \textit{white-box}, \textit{untargeted}, \textit{perturbative} attacks represent the expression of a threat model much more conceding to the \textit{adversary} \wrt their counterparts -- and more dangerous for the user (or the deployer) of the model.
It is additionally evident that a successful \textit{black-box} attack will succeed also in the respective \textit{white-box} setting (where, indeed, all the additional information would be simply not used in such case).

This may suggest the simplistic -- and only partially consistent -- conclusion that tackling \textit{white-box, untargeted, perturbative} attacks is enough to foil them all.

While this may approximate truth from a statistical viewpoint -- in the sense that a moderately large portion of \textit{black-box}-attacked inputs are within reach in the corresponding \textit{white-box} counterpart, and given the ability of a model specifically hardened against \textit{white-box} attacks to foil many \textit{black-box} analogues -- there still exist \textit{black-box} and/or \textit{targeted} techniques based on wildly different -- even unique -- mechanisms, both effective and hard to defend against, even specifically.

Additionally, even with a restriction to the exact (and only) threat model to be addressed, one is left with the Herculean task of determining which specific attacks to defend against (in both type and strength of the maximum allowed perturbation): the struggles are similar to those described right above for the threat model of choice -- with no clear-cut answers\footnote{But, as briefly discussed earlier, \textit{threat models} foreseeing \textit{universal attacks} should be favoured, by theory.}. This may give an example on how difficult a \textit{general solution} to the problem of adversarial robustness actually is, and on why even smaller progresses are cheered by the community.  The only possibility we are left with is to choose a combination of \textit{threat model}/\textit{type of attack}, try to develop new strategies to increase robustness in such scenario (\textit{i.e.} resulting in so-called \textit{foreseen attacks/settings}), and finally assess if any improvement also extends to other settings (\textit{i.e.} \textit{unforeseen attacks/settings}).

In such light, we can justify the adoption and study of the \textit{white-box, untargeted, perturbative} threat model (preferably if resulting in \textit{universal attacks}) as a reference -- the hardest to face -- but definitely not the exclusive in order to (try to) solve the problem of \textit{adversarial vulnerability}.
