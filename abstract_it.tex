% !TeX spellcheck = it

\begin{otherlanguage}{italian}

\newpage
\subsection{Abstract (italiano)}

Il \textit{deep learning} (ovvero \textit{machine learning} con \textit{reti neurali artificiali profonde}, principalmente nel loro \textit{regime fortemente sovraparametrizzato}) rappresenta l'epitome del machine learning contemporaneo dal punto di vista dell'\textit{accuratezza}, capace di ottenere risultati \textit{allo stato dell'arte} in un'ampia varietà di contesti, e di una diffusa e pur crescente adozione in ambito industriale, nel mercato di consumo e dei servizi. Ciononostante - e al punto da comprometterne l'applicabilità in scenari critici - tale paradigma può soffrire di significative debolezze, tra cui l'\textit{(assenza di)} \textit{adversarial robustness}: la possibilità per \textit{perturbazioni} degl'input costruite ad arte di alterare il funzionamento di un modello \textit{pre-allenato} rispetto ai corrispondenti input \textit{puliti} - e spesso pure rispetto alle aspettative informate dell'utente - in modo anche \textit{catastrofico}.

Nel contesto della \textit{classificazione d'immagini supervisionata} - su cui ci concentreremo nel presente lavoro - questo può essere addirittura rappresentato da una lieve aggiunta di rumore adeguatamente distribuito, impercettibile alla vista umana, ad un'immagine altrimenti \textit{legittima} e classificata correttamente, in grado di indurre un \textit{classificatore neurale} ad una classificazione errata con elevata confidenza. Oppure, all'estremo opposto dello spettro, un'immagine costituita all'apparenza da \textit{rumore bianco} in grado di essere classificata con elevata confidenza come una data classe-target, scelta a piacere dall'\textit{attaccante}.

Data la cruciale importanza di suddette vulnerabilità nel contesto della \textit{trustworthy artificial intelligence} - sviluppo di \textit{macchine in grado di apprendere} dei cui output ci si possa fidare, e loro irrobustimento contro manipolazioni o uso deliberatamente improprio - lo studio di questi fenomeni, con lo sviluppo di sempre nuovi \textit{attacchi} e \textit{difese}, è da qualche anno centrale all'interno della comunità dei ricercatori nell'ambito del deep learning. Tuttavia, il campo è in rapida evoluzione e - nonostante alcuni interessanti risultati in \textit{casi specifici} - non vi è ancora una soluzione universale e definitiva.

Nel lavoro in seguito sviluppato, viene proposta una nuova architettura per reti neurali artificiali, con associato \textit{protocollo di training e inferenza}, chiamata \textit{\texttt{CARSO}} (\textit{CounterAdversarial Recall of Synthetic Observations}), pensata per difendere contro \textit{adversarial attacks basati sul gradiente} nello scenario \textit{white box} (ovvero con l'attaccante in grado di utilizzare liberamente il modello, e accedere a pesi e gradienti). Tale proposta si configura come un'aggiunta facilmente inseribile all'interno di un'altra architettura preesistente (e pre-allenata). Pur richiedendo l'accesso ulteriore ad un dataset di immagini che \textit{si sappiano non perturbate} e ad un \textit{meccanismo di generazione d'attacchi} (un sottoinsieme dei requisiti del classico \textit{adversarial training}), tale tecnica è altrimenti completamente \textit{non-supervisionata} - consentendo di sfruttare anche grosse moli di dati la cui acquisizione sia reputata legittima, senza alcuno sforzo di \textit{labelling} ulteriore.

Nel corso della \textit{fase di addestramento} - immagini \textit{pulite}, e attacchi verso il classificatore pre-addestrato, sono prodotti e raccolti. A questo punto, le \textit{rappresentazioni interne} del classificatore prodotte da entrambe queste collezioni d'immagini sono utilizzate per \textit{condizionare} un \textit{conditional variational autoencoder} al fine d'imparare a mappare gli input (\textit{puliti} o perturbati che siano) verso il corrispondente \textit{pulito} di partenza (ovvero a copiare l'input, se \textit{pulito}; a produrre quello precedente la perturbazione, in caso contrario).

Questo processo può essere considerato l'equivalente non supervisionato dell'addestramento di un \textit{denoising class-conditional variational autoencoder} per la \textit{purificazione degli input}.

In fase d'inferenza - la rappresentazione prodotta da un nuovo input all'interno del classificatore pre-allenato è estratta, e in seguito utilizzata per condizionare un \textit{campionamento generativo} ripetuto nel decoder, ottenendo così una collezione di \textit{immagini tentativamente prive di rumore} associate a detta rappresentazione (e, per estensione, all'immagine di partenza in input). Su questa collezione il classificatore è quindi utilizzato \textit{convenzionalmente} e la \textit{classe moda} risultante restituita come output.

Vagamente ispirata al processo di \textit{richiamo attivo alla memoria} nel corso di attività di \textit{apprendimento visivo} negli animali, inclusi i primati e l'uomo, la tecnica appena descritta è in grado di proteggere efficacemente contro \textit{adversarial attacks white-box universali di primo ordine}, con una soltanto lieve perdita d'accuratezza, negli scenari considerati.

Rispetto ai risultati dell'ormai noto e rodato \textit{adversarial training iterativo} (dove \textit{tipo} e \textit{intensità} degli attacchi sono i medesimi in fase di addestramento e inferenza), \textit{\texttt{CARSO}} esce al peggio con discreto favore dal punto di vista dell'\textit{accuratezza sotto attacco}.

In aggiunta, un confronto significativamente favorevole si osserva nel momento in cui si considerano \textit{attacchi non osservati}, ovvero di intensità e/o tipo potenzialmente diversi da quelli considerati in \textit{training} - e la cui tipica conseguenza è solitamente la compromissione della \textit{adversarial robustness}, non raramente totale (cioè tale da produrre una accuratezza sotto attacco vicina allo zero). In occasioni specifiche, \textit{\texttt{CARSO}} contro attacchi \textit{non previsti} è stato in grado di esibire miglioramenti in termini d'accuratezza quasi paragonabili ad un suo recupero totale.

Da ultimo, la natura stocastica del \textit{campionamento generativo}, la non-differenziabilità dell'operazione di \textit{selezione della moda}, e la presenza di \textit{gradienti in competizione} a livello del classificatore pre-allenato (dal momento che sono determinati sia nel produrre la rappresentazione input per l'autoencoder, sia l'output del classificatore stesso) concorrono a rendere un eventuale attacco all'architettura stessa di \textit{\texttt{CARSO}} un problema di ottimizzazione vincolata multi-obiettivo, di assai difficile risoluzione. Questo consente di proteggere l'architettura aggiuntiva che \textit{\texttt{CARSO}} costituisce dalle stesse vulnerabilità che cerca di mitigare nel classificatore, in modo \textit{nativo}.

\end{otherlanguage}